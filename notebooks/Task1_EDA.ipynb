{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b132e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Task 1: Data Loading and Preparation...\n",
      "\n",
      "Successfully loaded stock data for: ['AAPL', 'AMZN', 'GOOG', 'META', 'MSFT', 'NVDA']\n",
      "\n",
      "AAPL Stock Data Head:\n",
      "               Close      High       Low      Open      Volume  Daily_Return\n",
      "Date                                                                        \n",
      "2009-01-02  2.721686  2.730385  2.554037  2.575630   746015200           NaN\n",
      "2009-01-05  2.836553  2.884539  2.780469  2.794266  1181608400      4.220416\n",
      "2009-01-06  2.789767  2.914229  2.770872  2.877641  1289310400     -1.649399\n",
      "2009-01-07  2.729484  2.774170  2.706990  2.753477   753048800     -2.160860\n",
      "2009-01-08  2.780169  2.793666  2.700393  2.712090   673500800      1.856959\n",
      "\n",
      "News file 'raw_analyst_ratings.csv' loaded. Applying date conversion fix...\n",
      "\n",
      "News Data Info after successful date conversion:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1407328 entries, 2009-02-14 00:00:00+00:00 to 2020-06-11 00:00:00+00:00\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count    Dtype \n",
      "---  ------      --------------    ----- \n",
      " 0   Unnamed: 0  1407328 non-null  int64 \n",
      " 1   headline    1407328 non-null  object\n",
      " 2   url         1407328 non-null  object\n",
      " 3   publisher   1407328 non-null  object\n",
      " 4   stock       1407328 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 64.4+ MB\n",
      "\n",
      "--- 3. Publisher Analysis ---\n",
      "Top 10 Publishers by Article Count:\n",
      "publisher\n",
      "Paul Quintaro        228373\n",
      "Lisa Levin           186979\n",
      "Benzinga Newsdesk    150484\n",
      "Charles Gross         96732\n",
      "Monica Gerson         82380\n",
      "Eddie Staley          57254\n",
      "Hal Lindon            49047\n",
      "ETF Professor         28489\n",
      "Juan Lopez            28438\n",
      "Benzinga Staff        28114\n",
      "\n",
      "--- 4. Sentiment Analysis ---\n",
      "\n",
      "Average Daily Sentiment Descriptive Stats:\n",
      "count    3955.000000\n",
      "mean        0.064820\n",
      "std         0.076491\n",
      "min        -0.447059\n",
      "25%         0.033440\n",
      "50%         0.050758\n",
      "75%         0.072252\n",
      "max         1.000000\n",
      "Name: Avg_Daily_Sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- Task 1: Exploratory Data Analysis (EDA) ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "# Uncomment and run the line below ONLY if you get an error that says 'punkt' is missing\n",
    "# nltk.download('punkt') \n",
    "\n",
    "# Define paths (assuming you are running this from the 'notebooks' folder)\n",
    "RAW_DATA_PATH = os.path.join('..', 'data', 'raw')\n",
    "PROCESSED_DATA_PATH = os.path.join('..', 'data', 'processed')\n",
    "TICKERS = ['AAPL', 'AMZN', 'GOOG', 'META', 'MSFT', 'NVDA']\n",
    "\n",
    "print(\"Starting Task 1: Data Loading and Preparation...\")\n",
    "\n",
    "# --- 1. Load All Stock Data ---\n",
    "all_stock_data = {}\n",
    "for ticker in TICKERS:\n",
    "    file_path = os.path.join(RAW_DATA_PATH, f'{ticker}.csv')\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Data Cleaning: Convert to datetime and set as index\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df = df.set_index('Date').sort_index()\n",
    "        \n",
    "        # Calculate Daily Returns (Core feature for Task 3)\n",
    "        df['Daily_Return'] = df['Close'].pct_change() * 100\n",
    "        \n",
    "        all_stock_data[ticker] = df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"WARNING: Stock file for {ticker} not found at {file_path}\")\n",
    "\n",
    "print(f\"\\nSuccessfully loaded stock data for: {list(all_stock_data.keys())}\")\n",
    "print(\"\\nAAPL Stock Data Head:\")\n",
    "print(all_stock_data['AAPL'].head())\n",
    "\n",
    "\n",
    "# --- 2. Load and Prepare News Data (FINAL ROBUST FIX) ---\n",
    "try:\n",
    "    NEWS_FILE = 'raw_analyst_ratings.csv' \n",
    "    df_news = pd.read_csv(os.path.join(RAW_DATA_PATH, NEWS_FILE))\n",
    "\n",
    "    print(f\"\\nNews file '{NEWS_FILE}' loaded. Applying date conversion fix...\")\n",
    "\n",
    "    # *** FINAL ROBUST FIX *** # This single line performs the conversion and normalization in the most stable way.\n",
    "    df_news['Date'] = pd.to_datetime(\n",
    "        df_news['date'], \n",
    "        format='mixed',           \n",
    "        errors='coerce',\n",
    "        utc=True              \n",
    "    ).dt.normalize()\n",
    "\n",
    "    # Drop the original string column and rows where date conversion failed (NaT values)\n",
    "    df_news = df_news.dropna(subset=['Date'])\n",
    "    df_news = df_news.drop(columns=['date']).set_index('Date').sort_index().copy() \n",
    "\n",
    "    print(\"\\nNews Data Info after successful date conversion:\")\n",
    "    df_news.info()\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"FATAL ERROR: News file not found. Check if the file name '{NEWS_FILE}' is correct and is in '{RAW_DATA_PATH}'.\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# --- The rest of the Task 1 analysis (Publisher, Sentiment, Visualization) ---\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# --- 3. Publisher Analysis ---\n",
    "print(\"\\n--- 3. Publisher Analysis ---\")\n",
    "publisher_counts = df_news['publisher'].value_counts()\n",
    "print(\"Top 10 Publishers by Article Count:\")\n",
    "print(publisher_counts.head(10).to_string())\n",
    "\n",
    "\n",
    "# --- 4. Sentiment Analysis (TextBlob) ---\n",
    "print(\"\\n--- 4. Sentiment Analysis ---\")\n",
    "\n",
    "def get_sentiment(text):\n",
    "    \"\"\"Calculates TextBlob sentiment polarity (range: -1 to 1).\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return 0.0\n",
    "    # TextBlob requires the input to be a string\n",
    "    return TextBlob(str(text)).sentiment.polarity\n",
    "\n",
    "# Apply sentiment analysis to all headlines\n",
    "df_news['sentiment_score'] = df_news['headline'].apply(get_sentiment)\n",
    "\n",
    "# Aggregate daily average sentiment\n",
    "daily_sentiment = df_news.groupby(df_news.index)['sentiment_score'].mean().rename('Avg_Daily_Sentiment')\n",
    "\n",
    "print(\"\\nAverage Daily Sentiment Descriptive Stats:\")\n",
    "print(daily_sentiment.describe())\n",
    "\n",
    "\n",
    "# --- 5. Visualization: Average Daily Sentiment  ---\n",
    "plt.figure(figsize=(14, 6))\n",
    "daily_sentiment.plot(\n",
    "    title='Average Daily News Sentiment Over Time',\n",
    "    color='darkorange',\n",
    "    linewidth=1.5\n",
    ")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Average Sentiment Score (-1: Negative, 1: Positive)\")\n",
    "# Maximize Data-Ink Ratio\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 6. Save Merged Sentiment Data for Task 3 ---\n",
    "# Ensure the processed data folder exists\n",
    "os.makedirs(PROCESSED_DATA_PATH, exist_ok=True)\n",
    "daily_sentiment.to_csv(os.path.join(PROCESSED_DATA_PATH, 'daily_sentiment.csv'))\n",
    "\n",
    "print(\"\\n--- Task 1: EDA Complete. Daily sentiment saved for Task 3. ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
